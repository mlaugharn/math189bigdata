\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{macros.tex}

% info for header block in upper right hand corner
\name{}
\class{Math189R SP19}
\assignment{Homework 4}
\duedate{Monday, Feb 25, 2019}

\begin{document}

Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
\newline
\newline
The starter files can be found under the Resource tab on course website. The graphs for problem 2 generated by the sample solution could be found in the corresponding zipfile. These graphs only serve as references to your implementation. You should generate your own graphs for submission. Please print out all the graphs generated by your own code and submit them together with the written part, and make sure you upload the code to your Github repository.

\begin{problem}[1]
\textbf{(Conditioning a Gaussian)} Note that from Murphy page 113. ``Equation 4.69
is of such importance in this book that we have put a box around it, so you can easily
find it.'' That equation is important. Read through the proof of the result.
Suppose we have a distribution over random variables $\xx = (\xx_1, \xx_2)$ that is
jointly Gaussian with parameters
\[
    \mub = \m{\mub_1\\\mub_2}\;\;\; \Sigmab = \m{\Sigmab_{11}&\Sigmab_{12}\\\Sigmab_{21}&\Sigmab_{22}},
\]
where
\[
    \mub_1 = \m{0\\0}, \;\; \mub_2 = 5, \;\; \Sigmab_{11} = \m{6 & 8\\ 8 & 13}, \;\; \Sigmab_{21}^\T = \Sigmab_{12} = \m{5\\11}, \;\; \Sigmab_{22} = \m{14}.
\]
Compute
\begin{enumerate}[(a)]
    \item The marginal distribution $p(\xx_1)$.
    \item The marginal distribution $p(\xx_2)$.
    \item The conditional distribution $p(\xx_1 | \xx_2)$
    \item The conditional distribution $p(\xx_2 | \xx_1)$
\end{enumerate}

\end{problem}
\begin{solution}
    \begin{enumerate}[(a)]
        \item $p(\xx_1) = \Nc(\xx_1 | \mub_1, \Sigmab_{11}) = \Nc(\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 6 & 8 \\ 8 & 13 \end{bmatrix})$
        \item $p(\xx_2) = \Nc(\xx_2 | \mub_2, \Sigmab_{22}) = \Nc(5, 14)$
        \item $p(\xx_1 | \xx_2) = \Nc(\xx_1 | \mub_{1|2}, \Sigmab_{1|2})$

            We have $\mub_{1|2} = \mub_1 + \Sigmab_{12} \Sigmab_{22}^-1 (\xx_2 - \mub_2)$

            and $\Sigmab_{1|2} = \Sigmab_{11} - \Sigmab_{12} \Sigmab_{22}^{-1} \Sigmab_{21}$

            So $\mub_{1|2} = \begin{bmatrix} 5 \\ 11 \end{bmatrix} \cdot \frac{1}{14} (\xx_2 - 5)$

            and $\Sigmab_{1|2} = \begin{bmatrix} 6 & 8 \\ 8 & 13 \end{bmatrix} - \begin{bmatrix} 5 \\ 11 \end{bmatrix} \cdot \frac{1}{14} \begin{bmatrix} 5 & 11 \end{bmatrix} = \frac{1}{14} \cdot \begin{bmatrix} 59 & 87 \\ -9 & 61 \end{bmatrix}$

        \item $p(\xx_2 | \xx_1) = \Nc(\xx_2 | \mub_{2|1}, \Sigmab_{2|1})$

            where $\mub_{2|1} = \mub_2 + \Sigmab_{21} \Sigmab_{11}^{-1} (\xx_1 - \mub_1) = 5 + \begin{bmatrix} -\frac{23}{14} & \frac{13}{7} \end{bmatrix} \xx_1$

            and

            $\Sigmab_{2|1} = \Sigmab_{22} - \Sigmab_{21} \Sigmab_{11}^{-1} \Sigmab_{12} = \frac{25}{14}$
    \end{enumerate}
\end{solution}
\newpage

\begin{problem}[2]
(\textbf{MNIST}) 
In this problem, we will use the MNIST dataset, a classic in the deep learning literature as a toy dataset to test
algorithms on, to set up a model for logistic regression and softmax regression. In the starter code, we have already parsed the data for you. However, you might need internet connection to access the data and therefore successfully run the starter code.
\newline
\newline
The problem is this: we have images of handwritten
digits with $28\times 28$ pixels in each image, as well as the label of which digit $0 \leq \texttt{label} \leq 9$ the written
digit corresponds to. Given a new image of a handwritten digit, we want to be
able to predict which digit it is.
The format of the data is \texttt{label, pix-11, pix-12, pix-13, ...}
where \texttt{pix-ij} is the pixel in the \texttt{ith} row and \texttt{jth} column.
\newline
\begin{enumerate}[(a)]
    \item (\textbf{logistic}) Restrict the dataset to only the digits with a label
        of 0 or 1. Implement L2 regularized logistic regression as a model to compute
        $\PP(y=1|\xx)$ for a different value of the regularization parameter $\lambda$.
        Plot the learning curve (objective vs. iteration) when using Newton's Method
        \textit{and} gradient descent.
        Plot the accuracy, precision ($p = \PP(y=1 | \hat y=1)$), recall ($r = \PP(\hat y=1 | y=1)$),
        and F1-score ($F1 = 2pr / (p+r)$) for different values of $\lambda$ (try at least
        10 different values including $\lambda = 0$) on the test set and report the
        value of $\lambda$ which maximizes the accuracy on the test set. What is your
        accuracy on the test set for this model? Your accuracy should definitely be
        over 90\%.

    \item (\textbf{softmax}) Now we will use the whole dataset and predict the label
        of each digit using L2 regularized softmax regression (multinomial logistic
        regression). Implement this using gradient descent, and plot the accuracy
        on the test set for different values of $\lambda$, the regularization parameter.
        Report the test accuracy for the optimal value of $\lambda$ as well as it's
        learning curve. Your accuracy should be over 90\%.

\end{enumerate}
\end{problem}
\begin{solution}
\begin{enumerate}[(a)]
    \item 
        From (8.3) we have

        $\text{NLL}(\ww) = -\sum\limits_{i} y_i \log \sigma(\ww^T \xx) + (1-y_i) \log (1 - \sigma(\ww^T \xx)) + \frac{\lambda}{2} {|| \ww ||}_2^2$

        and from (8.5) and (8.31) we have

        $\gg(\ww) = \Xb (\sigma(\Xb \ww) - \yy) + \lambda \ww$

        The Hessian is (using (8.7) and (8.32))

        $\Hb(\ww) = \Xb^T \Sb \Xb + \lambda \Ib = \Xb^T \text{diag} (\sigma(\Xb \ww) (1 - \sigma(\Xb \ww))) \Xb + \lambda \Ib$

        $\lambda = 5$ performs optimally
    \item 
        $\text{NLL}(\Wb) = -\prod_i \prod_c \mu_{ic}^{y_{ic}} + \lambda \tr (\Wb^T \Wb)$ (from 8.130) 

        $ = \sum_{i} \sum_c y_{i_c} \log \mu_{i_c} + \lambda \tr{\Wb^T \Wb}$
        Then 

        $\gg(\text{NLL}(\ww)) = \Xb^T (\mub - \yy) + \lambda \Wb$

    where $\mub_i = \frac{\exp \Wb^T \xx}{\sum_i (\exp \Wb^T \xx)_i}$ and $\yy$ is a one-hot encoding of representing the class of the output.

        For $\lambda = 0.01$ the accuracy was $0.9221$.
        
    \end{enumerate}
\end{solution}
\newpage

\end{document}
